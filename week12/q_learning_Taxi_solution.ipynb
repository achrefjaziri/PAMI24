{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q-learning-Taxi_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_SS21/blob/main/week10/q_learning_Taxi_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyizSSW2LDA"
      },
      "source": [
        "This notebook is based on coursera's Practical RL course by National Research University Higher School of Economics, https://www.coursera.org/learn/practical-rl/home/welcome\n",
        "\n",
        "We will make a Q-Learning agent to solve OpenAI Gym's Taxi problem.\n",
        "\n",
        "Q-Learning update equation:\n",
        "\n",
        "\n",
        "*   tabular: $$ Q(s,a) := (1 - \\alpha) \\cdot Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s')) \\\\ = Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s') - Q(s,a))$$\n",
        "\n",
        "For more definitions, see also https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdVjjTaIlms4"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQyTVzLTJva"
      },
      "source": [
        "We pick a simple test environment for q-learning: picking up and dropping off customers.\n",
        "\n",
        "Note that for the Taxi environment your reward can be negative, see link for details: https://gym.openai.com/envs/Taxi-v3/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRp87KGKu8Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73bc050-e407-49ab-daab-d276b525975a"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print('Number of discrete actions:', n_actions, ' - pick up, drop off, up, down, left, right')\n",
        "# observations\n",
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of discrete actions: 6  - pick up, drop off, up, down, left, right\n",
            "Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AruLHu_TYi4"
      },
      "source": [
        "Let's play a game (pick-up and drop off a customer) and see what happens if actions are just randomly sampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5JI_2WR1cSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "730c275d-eba9-4cb3-ff7e-fa7f83437596"
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    # sample an action\n",
        "    a = env.action_space.sample()\n",
        "    print('Action', a)\n",
        "    # get the reward and the next state and whether the game has finished\n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    # set the state to the next one\n",
        "    s = next_s\n",
        "    \n",
        "    print('reward: ',r)\n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 2\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Action 5\n",
            "reward:  -10\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Action 5\n",
            "reward:  -10\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Action 0\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Action 0\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Action 1\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Action 2\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Action 5\n",
            "reward:  -10\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 1\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Action 2\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Action 1\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Action 2\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R:\u001b[43m \u001b[0m| : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 5\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 5\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Action 1\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 1\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Action 3\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "Action 0\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Action 4\n",
            "reward:  -10\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fprOgg9ZTnd4"
      },
      "source": [
        "Let's now build an agent which will learn a better policy to pick up customers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nsw7Ex0Kuf"
      },
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on http://ai.berkeley.edu/projects/release/reinforcement/v1/001/docs/qlearningAgents.html\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        # dictionary of expected rewards for (state,action) pairs\n",
        "        # it is a dictionary of dictionaries, because for every state,\n",
        "        # there could be multiple actions, each of which has qvalue\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        # learning rate\n",
        "        self.alpha = alpha\n",
        "        # exploration-exploitation trade-off\n",
        "        self.epsilon = epsilon\n",
        "        # gamma - the future reward discount factor\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # go through all possible actions, check the qvalue and return the max\n",
        "        \n",
        "        value = -np.inf\n",
        "        \n",
        "        for action in possible_actions:\n",
        "            tmp_val = self.get_qvalue(state, action)\n",
        "            if value < tmp_val:\n",
        "                value = tmp_val\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # analog to the get_value method, with a different return\n",
        "        \n",
        "        value = -np.inf\n",
        "        best_action = None                                                                          \n",
        "        \n",
        "        for action in possible_actions:\n",
        "            tmp_val =self. get_qvalue(state, action)\n",
        "            if value < tmp_val:\n",
        "                value = tmp_val\n",
        "                best_action = action\n",
        "                                                                                  \n",
        "        return best_action\n",
        "        \n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action.\n",
        "        \n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        # agent parameters:\n",
        "        epsilon = self.epsilon\n",
        "        pick_best = np.random.rand() >= epsilon\n",
        "        \n",
        "        if pick_best:                                                                      \n",
        "            chosen_action = self.get_best_action(state)\n",
        "        else:\n",
        "            chosen_action = np.random.choice(possible_actions)                                                                      \n",
        "        \n",
        "        return chosen_action\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "           \n",
        "        Hint: use get_value method\n",
        "        \"\"\"\n",
        "\n",
        "        #agent parameters\n",
        "        gamma = self.discount\n",
        "        learning_rate = self.alpha\n",
        "        \n",
        "        value = (1 - learning_rate) * self.get_qvalue(state, action) \n",
        "        value += learning_rate * (reward + gamma * self.get_value(next_state))\n",
        "        \n",
        "        self.set_qvalue(state, action, value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sBB3ZbsSqQ"
      },
      "source": [
        "# initialize a new agent\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AysnffDtLF"
      },
      "source": [
        "# update at each train step\n",
        "def play_and_train(env,agent,t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s, get new state and reward, then update your agent\n",
        "        a = agent.get_action(s)\n",
        "        \n",
        "        next_s, r, done, _ = env.step(a)\n",
        "        \n",
        "        # train (update) agent for state s\n",
        "        agent.update(s, a, r, next_s, done)\n",
        "        \n",
        "        s = next_s\n",
        "        total_reward +=r\n",
        "        if done: break\n",
        "        \n",
        "    return total_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5lHa_KZybJ"
      },
      "source": [
        "Let us visualize the reward: play a game and update the agent, dicrease exploration (since the agent has already learned something) and plot the mean reward over the last ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJeVP7SoVguN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "0ea5f6ec-203a-480b-c7a5-138ab6a29389"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    # we are making epsilon smaller \n",
        "    agent.epsilon *= 0.99\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eps = 2.9191091959171894e-05 mean reward = 6.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw17Mw2Wcya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aad9068-ed97-4d37-9a0b-0d768614f395"
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    a = agent.get_best_action(s)\n",
        "        \n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    \n",
        "    s = next_s\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : |\u001b[42m_\u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}