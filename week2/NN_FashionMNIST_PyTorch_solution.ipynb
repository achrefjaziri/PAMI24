{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
 {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccc-frankfurt/Practical_ML_WS19/blob/master/week5/MLP_Numpy_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxZXfBj-ZwXG"
      },
      "source": [
        "# Neural Networks for Fashion MNIST in PyTorch\n",
        "We will extend our previous MLP from scratch example by re-implementing the same content in PyTorch. This may seem like a tour-de-force, but will show just exactly how much of the complicated underlying implementation is abstracted away from the user in modern Deep Learning frameworks. We will then proceed to implement a simple convolutional neural network (CNN).\n",
        "\n",
        "Luckily, PyTorch is already installed by default in Colab. We will install one auxiliary package called torchnet: https://github.com/pytorch/tnt though, which we will use for confusion matrices.\n",
        "\n",
        "Before starting the notebook you should make sure that your runtime uses GPU acceleration. You can find the corresponding option under *runtime* and then *change runtime type*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbvMRv4aHMj",
        "outputId": "db8ed52d-c866-482d-ff4f-c236766f7922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torchnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchnet in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchnet) (2.2.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchnet) (1.16.0)\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.10/dist-packages (from torchnet) (0.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchnet) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchnet) (12.4.127)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.11.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (6.3.3)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (1.7.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom->torchnet) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchnet) (2.1.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch->visdom->torchnet) (2.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom->torchnet) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchnet) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fR1gvRs2ukp"
      },
      "source": [
        "As always we will import numpy as we will still use it for our dataloader. We now also import PyTorch (simply called torch when importing) and particularly its neural network specific part *nn*. To be on the safe side you can also print Colab's pre-installed version of PyTorch and check if it corresponds to the most recent version (or alternatively update it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-P51_uaXIi",
        "outputId": "bb148aa1-7c2c-4383-aa9c-96a26b9829d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "NB7ZruPuZwXH"
      },
      "source": [
        "### Dataset class extended to use directly in PyTorch\n",
        "We can basically take our given dataset loader from our previous MLP from scratch FashionMNIST example and use it almost as is.\n",
        "\n",
        "There is one modification that we absolutely have to make which is converting the numpy arrays to torch tensors.\n",
        "Below, we will have to use the function *torch.from_numpy()* for this purpose.\n",
        "\n",
        "Two additional features we can add is the use of PyTorch dataset and dataloader structures that are very convenient to use and highly efficient.\n",
        "These are called *torch.utils.data.TensorDataset* and *torch.utils.data.DataLoader* and allow for the use of a multi-threaded mini-batch dataset loader. In contrast to storing the entire dataset in our memory, this data loader allows us to only load and return the current mini-batch and e.g. store the rest of the dataset in terms of paths only. Although we can just load the entire dataset at once in our simple example (in fact we still do when loading it into Numpy the first time), this is particularly useful for large datasets thart do not fit our memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import struct\n",
        "import gzip\n",
        "import errno\n",
        "import torch.utils.data\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize around 0.5 since the input is [0, 1]\n",
        "])\n",
        "# Load the datasets\n",
        "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "val_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "class_to_idx = {'T-shirt/top': 0,\n",
        "                'Trouser': 1,\n",
        "                'Pullover': 2,\n",
        "                'Dress': 3,\n",
        "                'Coat': 4,\n",
        "                'Sandal': 5,\n",
        "                'Shirt': 6,\n",
        "                'Sneaker': 7,\n",
        "                'Bag': 8,\n",
        "                'Ankle boot': 9}"
      ],
      "metadata": {
        "id": "unBX-X-Y8Tmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters\n",
        "batch_size = 128  # number of data samples processed in each batch\n",
        "shuffle = True   # shuffle the data\n",
        "num_workers = 4  # number of subprocesses to use for data loading\n",
        "is_gpu = torch.cuda.is_available()  # use GPU if available\n",
        "\n",
        "print(\"GPU is available:\", is_gpu)\n",
        "print(\"If you are receiving False, try setting your runtime to GPU\")\n",
        "device = torch.device(\"cuda\" if is_gpu else \"cpu\")\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=shuffle,\n",
        "                          num_workers=num_workers, pin_memory=is_gpu)\n",
        "test_loader = DataLoader(val_set, batch_size=batch_size, shuffle=shuffle,\n",
        "                         num_workers=num_workers, pin_memory=is_gpu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHc6FLu28lcf",
        "outputId": "23ae42b6-7b66-463c-cc5f-2b55858728c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available: True\n",
            "If you are receiving False, try setting your runtime to GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuIk7hjj5Xbl"
      },
      "source": [
        "Let's load the data and set the device to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKpmq-3VZwXP"
      },
      "source": [
        "### The MLP model in PyTorch\n",
        "We now show how to implement a 2 hidden layer MLP in PyTorch.\n",
        "\n",
        "Suitable hidden-layer sizes for this task could be 100 and 100, like in our last notebook.\n",
        "Because we are using an optimized GPU implementation, you are welcome and should try larger sizes to see the impact of neural network size (capacity) on our task!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JXFkMigZwXR"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, img_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # we can optionally set the \"bias=False\"\n",
        "        self.fc1 = nn.Linear(img_size, 100)\n",
        "        self.fc2 = nn.Linear(100, 100)\n",
        "        self.fc3 = nn.Linear(100, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The view flattens the data to a vector (the representation needed by the MLP)\n",
        "        x = x.view(-1, self.img_size)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxw9nHKPZwXT"
      },
      "source": [
        "### Defining optimization criterion and optimizer\n",
        "A good baseline is a Cross Entropy loss (that combines a logarithmic Softmax + negative log-likelihood) and a stochastic gradient descent (SGD) algorithm with a baseline learning rate of 0.01.\n",
        "The Softmax function: https://en.wikipedia.org/wiki/Softmax_function is similar to the Sigmoid function, but is a normalized exponential and thus normalizes the probability of the output. In contrast to the Sigmoid unit that just gives out values in the range of 0-1 for each output unit, the Softmax function outputs values that are normalized to 1 across the entire range of all outputs.\n",
        "\n",
        "\n",
        "If we want to we can use additional momenta or regularization terms (such as L2 - Tikhonov regularization commonly reffered to as weight-decay in ML). The respective optimizer parameters are called *momentum* and *weight_decay*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHI6LGtuZwXU",
        "outputId": "4b1bf74e-3bc3-45aa-e925-d398e1b43cb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define optimizer and loss function (criterion)\n",
        "img_size = 28 * 28\n",
        "num_classes = 10\n",
        "\n",
        "# create an instance of the MLP and transfer the model to the device.\n",
        "# Note that we do not necessarily need any custom weight initialization as PyTorch\n",
        "# already uses the initialization schemes that we have previously learned about internally.\n",
        "model = MLP(img_size, num_classes).to(device)\n",
        "# we can also print the model architecture\n",
        "print(model)\n",
        "\n",
        "# set the loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# we can use advanced stochastic gradient descent algorithms\n",
        "# with regularization (weight-decay) or momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=5e-4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1Bei8DZwXW"
      },
      "source": [
        "### Monitoring and calculating accuracy\n",
        "We add a convenience class to keep track and average concepts such as processing or data loading speeds, losses and accuracies. For this we need to define a function to define accuracy, which could be based on the absolute accuracy, or top-1 accuracy. Often times in Machine Learning other metrics are employed. For example, in the ImageNet ILSVRC challenge with a classification problem containing 1000 classes, it is common to report the top-5 accuracy. Here a prediction is counted as accurate if the correct class lies within the top-5 most likely output classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajy_8OckZwXX"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Evaluates a model's top k accuracy\n",
        "\n",
        "    Parameters:\n",
        "        output (torch.autograd.Variable): model output\n",
        "        target (torch.autograd.Variable): ground-truths/labels\n",
        "        topk (list): list of integers specifying top-k precisions\n",
        "            to be computed\n",
        "\n",
        "    Returns:\n",
        "        float: percentage of correct predictions\n",
        "    \"\"\"\n",
        "\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "\n",
        "        #correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIqY9nJOZwXa"
      },
      "source": [
        "### Training function (sometimes referred to as \"hook\")\n",
        "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a *backward* pass is done to calculate gradients and an *optimizer step* is done in order to do the respective update to the model's weights. This is similar to our former notebook where we first calculate the errors/deltas for every layer and then apply the weight updates at the end.\n",
        "\n",
        "When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In contrast to our previous notebook from scratch, in this implementation this is handled by the \"sampler\" of the dataset loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hyNEtLtZwXa"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Trains/updates the model for one epoch on the training dataset.\n",
        "\n",
        "    Parameters:\n",
        "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
        "        model (torch.nn.module): Model to be trained\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create instances of the average meter to track losses and accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate through the dataset loader\n",
        "    for i, (inp, target) in enumerate(train_loader):\n",
        "        # transfer inputs and targets to the GPU (if it is available)\n",
        "        inp = inp.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # compute output, i.e. the model forward\n",
        "        output = model(inp)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss and accuracy\n",
        "        prec1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), inp.size(0))\n",
        "        top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "        # compute gradient and do the SGD step\n",
        "        # we reset the optimizer with zero_grad to \"flush\" former gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print the loss every 100 mini-batches\n",
        "        if i % 100 == 0:\n",
        "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                   loss=losses, top1=top1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YpNuUY0ZwXd"
      },
      "source": [
        "### Validation function\n",
        "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zihygz7tZwXd"
      },
      "source": [
        "from torchnet import meter\n",
        "\n",
        "def validate(val_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates/validates the model\n",
        "\n",
        "    Parameters:\n",
        "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
        "        model (torch.nn.module): Model to be evaluated/validated\n",
        "        criterion (torch.nn.criterion): Loss function\n",
        "        device (string): cuda or cpu\n",
        "    \"\"\"\n",
        "\n",
        "    # create instances of the average meter to track losses and accuracies\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "\n",
        "    confusion = meter.ConfusionMeter(len(class_to_idx))\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    # (this would be important for e.g. dropout where stochasticity shouldn't be applied during testing)\n",
        "    model.eval()\n",
        "\n",
        "    # avoid computation of gradients and necessary storing of intermediate layer activations\n",
        "    with torch.no_grad():\n",
        "        # iterate through the dataset loader\n",
        "        for i, (inp, target) in enumerate(val_loader):\n",
        "            # transfer to device\n",
        "            inp = inp.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # compute output\n",
        "            output = model(inp)\n",
        "\n",
        "            # compute loss\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss and accuracy\n",
        "            prec1, _ = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), inp.size(0))\n",
        "            top1.update(prec1.item(), inp.size(0))\n",
        "\n",
        "            # add to confusion matrix\n",
        "            confusion.add(output.data, target)\n",
        "\n",
        "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW-sqbUaZwXi"
      },
      "source": [
        "### Running the training of the model\n",
        "Let's optimize this model for 20 epochs and check at every epoch how we are doing on our validation set.\n",
        "\n",
        "Depending on your model definition and optimizer you might experience over-fitting!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFvFnowLZwXj",
        "outputId": "14960e2c-29e6-4d74-9dc3-19affe946400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(train_loader, model, criterion, optimizer, device)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(test_loader, model, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1\n",
            "TRAIN\n",
            "Loss 2.2830 (2.2830)\tPrec@1 11.719 (11.719)\n",
            "Loss 0.7795 (1.1790)\tPrec@1 61.719 (58.540)\n",
            "Loss 0.6281 (0.8943)\tPrec@1 80.469 (68.140)\n",
            "Loss 0.4789 (0.7738)\tPrec@1 79.688 (72.228)\n",
            "Loss 0.4744 (0.7009)\tPrec@1 76.562 (74.819)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 82.210 \n",
            "EPOCH: 2\n",
            "TRAIN\n",
            "Loss 0.5298 (0.5298)\tPrec@1 74.219 (74.219)\n",
            "Loss 0.4464 (0.4450)\tPrec@1 81.250 (83.965)\n",
            "Loss 0.5041 (0.4378)\tPrec@1 83.594 (84.146)\n",
            "Loss 0.3916 (0.4319)\tPrec@1 85.156 (84.422)\n",
            "Loss 0.3503 (0.4277)\tPrec@1 85.938 (84.554)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 83.960 \n",
            "EPOCH: 3\n",
            "TRAIN\n",
            "Loss 0.5885 (0.5885)\tPrec@1 78.906 (78.906)\n",
            "Loss 0.3482 (0.3923)\tPrec@1 89.844 (85.876)\n",
            "Loss 0.3421 (0.3889)\tPrec@1 88.281 (85.825)\n",
            "Loss 0.4317 (0.3852)\tPrec@1 86.719 (86.031)\n",
            "Loss 0.3192 (0.3834)\tPrec@1 89.062 (86.091)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 85.030 \n",
            "EPOCH: 4\n",
            "TRAIN\n",
            "Loss 0.5241 (0.5241)\tPrec@1 82.031 (82.031)\n",
            "Loss 0.3474 (0.3579)\tPrec@1 88.281 (87.028)\n",
            "Loss 0.3267 (0.3600)\tPrec@1 87.500 (86.894)\n",
            "Loss 0.4316 (0.3616)\tPrec@1 78.906 (86.807)\n",
            "Loss 0.3697 (0.3638)\tPrec@1 83.594 (86.697)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 85.020 \n",
            "EPOCH: 5\n",
            "TRAIN\n",
            "Loss 0.4511 (0.4511)\tPrec@1 84.375 (84.375)\n",
            "Loss 0.2504 (0.3463)\tPrec@1 90.625 (87.446)\n",
            "Loss 0.2635 (0.3432)\tPrec@1 93.750 (87.469)\n",
            "Loss 0.3141 (0.3365)\tPrec@1 89.062 (87.710)\n",
            "Loss 0.3837 (0.3358)\tPrec@1 82.812 (87.773)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 86.700 \n",
            "EPOCH: 6\n",
            "TRAIN\n",
            "Loss 0.3365 (0.3365)\tPrec@1 86.719 (86.719)\n",
            "Loss 0.3222 (0.3121)\tPrec@1 88.281 (88.451)\n",
            "Loss 0.4283 (0.3260)\tPrec@1 81.250 (87.978)\n",
            "Loss 0.2405 (0.3234)\tPrec@1 91.406 (88.074)\n",
            "Loss 0.2524 (0.3257)\tPrec@1 90.625 (87.956)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 86.370 \n",
            "EPOCH: 7\n",
            "TRAIN\n",
            "Loss 0.3002 (0.3002)\tPrec@1 86.719 (86.719)\n",
            "Loss 0.3232 (0.3019)\tPrec@1 89.844 (88.660)\n",
            "Loss 0.2926 (0.3133)\tPrec@1 90.625 (88.328)\n",
            "Loss 0.2983 (0.3098)\tPrec@1 91.406 (88.569)\n",
            "Loss 0.2527 (0.3113)\tPrec@1 89.844 (88.505)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.040 \n",
            "EPOCH: 8\n",
            "TRAIN\n",
            "Loss 0.2123 (0.2123)\tPrec@1 93.750 (93.750)\n",
            "Loss 0.3166 (0.2987)\tPrec@1 86.719 (88.854)\n",
            "Loss 0.3264 (0.3045)\tPrec@1 88.281 (88.833)\n",
            "Loss 0.2643 (0.3003)\tPrec@1 89.062 (89.031)\n",
            "Loss 0.2832 (0.3024)\tPrec@1 89.844 (88.881)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.100 \n",
            "EPOCH: 9\n",
            "TRAIN\n",
            "Loss 0.2773 (0.2773)\tPrec@1 91.406 (91.406)\n",
            "Loss 0.2755 (0.2921)\tPrec@1 88.281 (89.333)\n",
            "Loss 0.3826 (0.2912)\tPrec@1 85.156 (89.467)\n",
            "Loss 0.1937 (0.2908)\tPrec@1 92.188 (89.452)\n",
            "Loss 0.2753 (0.2913)\tPrec@1 92.188 (89.466)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 86.940 \n",
            "EPOCH: 10\n",
            "TRAIN\n",
            "Loss 0.2639 (0.2639)\tPrec@1 90.625 (90.625)\n",
            "Loss 0.2711 (0.2817)\tPrec@1 89.844 (89.179)\n",
            "Loss 0.2591 (0.2827)\tPrec@1 89.844 (89.311)\n",
            "Loss 0.3182 (0.2823)\tPrec@1 89.844 (89.441)\n",
            "Loss 0.2215 (0.2820)\tPrec@1 92.188 (89.483)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.810 \n",
            "EPOCH: 11\n",
            "TRAIN\n",
            "Loss 0.2121 (0.2121)\tPrec@1 90.625 (90.625)\n",
            "Loss 0.1967 (0.2697)\tPrec@1 92.969 (90.184)\n",
            "Loss 0.2448 (0.2699)\tPrec@1 89.062 (90.093)\n",
            "Loss 0.1662 (0.2728)\tPrec@1 95.312 (90.085)\n",
            "Loss 0.3006 (0.2721)\tPrec@1 89.062 (90.105)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.870 \n",
            "EPOCH: 12\n",
            "TRAIN\n",
            "Loss 0.3013 (0.3013)\tPrec@1 85.938 (85.938)\n",
            "Loss 0.3561 (0.2661)\tPrec@1 87.500 (90.176)\n",
            "Loss 0.2627 (0.2660)\tPrec@1 89.844 (90.194)\n",
            "Loss 0.3587 (0.2661)\tPrec@1 83.594 (90.158)\n",
            "Loss 0.2320 (0.2668)\tPrec@1 88.281 (90.128)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.570 \n",
            "EPOCH: 13\n",
            "TRAIN\n",
            "Loss 0.2463 (0.2463)\tPrec@1 91.406 (91.406)\n",
            "Loss 0.2625 (0.2649)\tPrec@1 90.625 (90.385)\n",
            "Loss 0.3268 (0.2627)\tPrec@1 88.281 (90.435)\n",
            "Loss 0.3373 (0.2631)\tPrec@1 87.500 (90.404)\n",
            "Loss 0.2006 (0.2639)\tPrec@1 92.969 (90.319)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.730 \n",
            "EPOCH: 14\n",
            "TRAIN\n",
            "Loss 0.2774 (0.2774)\tPrec@1 91.406 (91.406)\n",
            "Loss 0.2480 (0.2497)\tPrec@1 92.188 (90.965)\n",
            "Loss 0.2232 (0.2525)\tPrec@1 91.406 (90.843)\n",
            "Loss 0.1868 (0.2520)\tPrec@1 92.969 (90.768)\n",
            "Loss 0.2359 (0.2547)\tPrec@1 90.625 (90.625)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.940 \n",
            "EPOCH: 15\n",
            "TRAIN\n",
            "Loss 0.2978 (0.2978)\tPrec@1 89.062 (89.062)\n",
            "Loss 0.2811 (0.2424)\tPrec@1 87.500 (91.120)\n",
            "Loss 0.2374 (0.2501)\tPrec@1 93.750 (90.812)\n",
            "Loss 0.1691 (0.2505)\tPrec@1 93.750 (90.765)\n",
            "Loss 0.1720 (0.2510)\tPrec@1 96.094 (90.717)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.750 \n",
            "EPOCH: 16\n",
            "TRAIN\n",
            "Loss 0.2030 (0.2030)\tPrec@1 92.188 (92.188)\n",
            "Loss 0.1428 (0.2374)\tPrec@1 95.312 (91.476)\n",
            "Loss 0.2839 (0.2412)\tPrec@1 88.281 (91.227)\n",
            "Loss 0.1708 (0.2410)\tPrec@1 94.531 (91.178)\n",
            "Loss 0.2358 (0.2410)\tPrec@1 91.406 (91.188)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.130 \n",
            "EPOCH: 17\n",
            "TRAIN\n",
            "Loss 0.2285 (0.2285)\tPrec@1 89.844 (89.844)\n",
            "Loss 0.2812 (0.2387)\tPrec@1 87.500 (91.166)\n",
            "Loss 0.2497 (0.2352)\tPrec@1 90.625 (91.465)\n",
            "Loss 0.3792 (0.2391)\tPrec@1 90.625 (91.261)\n",
            "Loss 0.1876 (0.2409)\tPrec@1 93.750 (91.169)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.340 \n",
            "EPOCH: 18\n",
            "TRAIN\n",
            "Loss 0.2938 (0.2938)\tPrec@1 89.844 (89.844)\n",
            "Loss 0.2055 (0.2212)\tPrec@1 95.312 (91.932)\n",
            "Loss 0.1426 (0.2304)\tPrec@1 96.094 (91.651)\n",
            "Loss 0.2070 (0.2307)\tPrec@1 90.625 (91.528)\n",
            "Loss 0.2215 (0.2342)\tPrec@1 92.969 (91.332)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.630 \n",
            "EPOCH: 19\n",
            "TRAIN\n",
            "Loss 0.2322 (0.2322)\tPrec@1 89.062 (89.062)\n",
            "Loss 0.1953 (0.2259)\tPrec@1 92.188 (91.863)\n",
            "Loss 0.2121 (0.2295)\tPrec@1 94.531 (91.647)\n",
            "Loss 0.2205 (0.2304)\tPrec@1 91.406 (91.546)\n",
            "Loss 0.2288 (0.2311)\tPrec@1 91.406 (91.445)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.270 \n",
            "EPOCH: 20\n",
            "TRAIN\n",
            "Loss 0.1681 (0.1681)\tPrec@1 93.750 (93.750)\n",
            "Loss 0.2514 (0.2175)\tPrec@1 89.844 (92.087)\n",
            "Loss 0.2256 (0.2211)\tPrec@1 91.406 (91.678)\n",
            "Loss 0.2151 (0.2241)\tPrec@1 92.969 (91.578)\n",
            "Loss 0.1604 (0.2253)\tPrec@1 92.969 (91.562)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.290 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "kldGeVaXZwXo"
      },
      "source": [
        "### Moving from MLP to CNN\n",
        "Now that we have seen how our two-hidden layer MLP performs, let's see how we can move on to a convolutional neural network (CNN). The advantage of a CNN is that the we no longer have an all-to-all connectivity structure between layers, but rather take a look at local (2-D or even 3-D) neighborhoods. This spatial (or even temporal) filter is then convolved over the whole input (here an image) by \"sharing the weights\" to every position. The outcome is typically referred to as a feature map and in order to check for multiple features we apply a set of such filters in parallel.  We will see how these effects improve our accuracy in contrast to our MLP.\n",
        "\n",
        "Let us see how to build a CNN with 2 layers with a fully-connected classifier on top and included pooling layers after every convolution. These layers generally subsample the input and introduce translation invariance (to an extent). The network should again have rectified linear units for activation functions and end on a fully-connected linear layer to the amount of classes.\n",
        "\n",
        "    1. Define two convolution layers \"nn.Conv2d\" with 5 x 5 filters. Good starting values for amount of filters/features can be 64 in the first and 128 in the second layer.\n",
        "    2. Convolutions should be followed by ReLU activations. You can apply the activations in the definition of the forward pass with the functional package and \"F.relu\"\n",
        "    3. Each conv + act block should be followed by a 2 x 2 max pooling \"nn.MaxPool2d\" with stride 2.\n",
        "    4. You will need to calculate the remaining spatial dimensionality to flatten the convolutional output to feed it to the last fully-connected layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4bPPd5uZwXo"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, 5) # input features, output features, kernel size\n",
        "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 5) # input features, output features, kernel size\n",
        "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
        "\n",
        "        self.fc = nn.Linear(128*4*4, num_classes) # 4x4 is the remaining spatial resolution here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv + ReLU + max pooling for two layers\n",
        "        x = self.mp1(F.relu(self.conv1(x)))\n",
        "        x = self.mp2(F.relu(self.conv2(x)))\n",
        "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
        "        x = x.view(-1, 128*4*4)\n",
        "        # apply fully-connected linear layer\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWBpW7QEZwXq"
      },
      "source": [
        "### Constructing and running the CNN\n",
        "Let's create an instance of our CNN model and optimize it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CfyFWGXZwXr",
        "outputId": "793f495e-247f-489b-dd11-25c94be69255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create CNN model instance\n",
        "model = CNN(num_classes).to(device)\n",
        "print(model)\n",
        "\n",
        "# again, define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=5e-4)\n",
        "\n",
        "# optimize\n",
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(train_loader, model, criterion, optimizer, device)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(test_loader, model, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (mp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (mp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            ")\n",
            "EPOCH: 1\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 2.2971 (2.2971)\tPrec@1 7.031 (7.031)\n",
            "Loss 0.6193 (0.9275)\tPrec@1 72.656 (68.502)\n",
            "Loss 0.5270 (0.7262)\tPrec@1 82.812 (74.728)\n",
            "Loss 0.3597 (0.6335)\tPrec@1 88.281 (77.889)\n",
            "Loss 0.4488 (0.5790)\tPrec@1 79.688 (79.760)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 84.050 \n",
            "EPOCH: 2\n",
            "TRAIN\n",
            "Loss 0.4230 (0.4230)\tPrec@1 87.500 (87.500)\n",
            "Loss 0.3309 (0.3867)\tPrec@1 85.938 (86.015)\n",
            "Loss 0.4547 (0.3719)\tPrec@1 84.375 (86.762)\n",
            "Loss 0.2766 (0.3616)\tPrec@1 91.406 (87.067)\n",
            "Loss 0.3123 (0.3571)\tPrec@1 89.062 (87.227)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 87.940 \n",
            "EPOCH: 3\n",
            "TRAIN\n",
            "Loss 0.2651 (0.2651)\tPrec@1 92.969 (92.969)\n",
            "Loss 0.3322 (0.3124)\tPrec@1 89.844 (88.800)\n",
            "Loss 0.2921 (0.3080)\tPrec@1 91.406 (88.996)\n",
            "Loss 0.2414 (0.3062)\tPrec@1 90.625 (89.068)\n",
            "Loss 0.3339 (0.3043)\tPrec@1 89.844 (89.137)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.260 \n",
            "EPOCH: 4\n",
            "TRAIN\n",
            "Loss 0.2944 (0.2944)\tPrec@1 89.062 (89.062)\n",
            "Loss 0.2131 (0.2775)\tPrec@1 89.844 (89.937)\n",
            "Loss 0.2898 (0.2787)\tPrec@1 89.062 (90.038)\n",
            "Loss 0.2546 (0.2775)\tPrec@1 91.406 (90.111)\n",
            "Loss 0.3039 (0.2786)\tPrec@1 88.281 (90.111)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 89.450 \n",
            "EPOCH: 5\n",
            "TRAIN\n",
            "Loss 0.1904 (0.1904)\tPrec@1 92.188 (92.188)\n",
            "Loss 0.2230 (0.2599)\tPrec@1 92.188 (90.602)\n",
            "Loss 0.2223 (0.2548)\tPrec@1 92.188 (90.854)\n",
            "Loss 0.1942 (0.2542)\tPrec@1 91.406 (91.012)\n",
            "Loss 0.2816 (0.2550)\tPrec@1 90.625 (90.989)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 89.880 \n",
            "EPOCH: 6\n",
            "TRAIN\n",
            "Loss 0.3706 (0.3706)\tPrec@1 86.719 (86.719)\n",
            "Loss 0.3000 (0.2349)\tPrec@1 89.844 (92.017)\n",
            "Loss 0.2422 (0.2360)\tPrec@1 89.844 (91.791)\n",
            "Loss 0.2064 (0.2397)\tPrec@1 92.969 (91.645)\n",
            "Loss 0.2080 (0.2396)\tPrec@1 92.188 (91.646)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 89.870 \n",
            "EPOCH: 7\n",
            "TRAIN\n",
            "Loss 0.2928 (0.2928)\tPrec@1 90.625 (90.625)\n",
            "Loss 0.2586 (0.2191)\tPrec@1 89.062 (91.925)\n",
            "Loss 0.2202 (0.2252)\tPrec@1 92.188 (91.861)\n",
            "Loss 0.2205 (0.2261)\tPrec@1 91.406 (91.907)\n",
            "Loss 0.2598 (0.2241)\tPrec@1 91.406 (91.967)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.120 \n",
            "EPOCH: 8\n",
            "TRAIN\n",
            "Loss 0.3011 (0.3011)\tPrec@1 89.062 (89.062)\n",
            "Loss 0.2242 (0.2120)\tPrec@1 92.188 (92.597)\n",
            "Loss 0.2015 (0.2143)\tPrec@1 91.406 (92.390)\n",
            "Loss 0.2293 (0.2127)\tPrec@1 91.406 (92.398)\n",
            "Loss 0.2457 (0.2141)\tPrec@1 90.625 (92.376)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.870 \n",
            "EPOCH: 9\n",
            "TRAIN\n",
            "Loss 0.2125 (0.2125)\tPrec@1 91.406 (91.406)\n",
            "Loss 0.2244 (0.1945)\tPrec@1 91.406 (93.139)\n",
            "Loss 0.1532 (0.1982)\tPrec@1 92.188 (93.074)\n",
            "Loss 0.2365 (0.1982)\tPrec@1 89.844 (92.984)\n",
            "Loss 0.2335 (0.2014)\tPrec@1 92.188 (92.891)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.170 \n",
            "EPOCH: 10\n",
            "TRAIN\n",
            "Loss 0.2194 (0.2194)\tPrec@1 91.406 (91.406)\n",
            "Loss 0.1728 (0.1947)\tPrec@1 92.969 (93.085)\n",
            "Loss 0.1384 (0.1940)\tPrec@1 95.312 (93.167)\n",
            "Loss 0.0929 (0.1934)\tPrec@1 96.094 (93.197)\n",
            "Loss 0.2171 (0.1917)\tPrec@1 92.969 (93.245)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.080 \n",
            "EPOCH: 11\n",
            "TRAIN\n",
            "Loss 0.1198 (0.1198)\tPrec@1 96.094 (96.094)\n",
            "Loss 0.1326 (0.1795)\tPrec@1 94.531 (93.634)\n",
            "Loss 0.1437 (0.1779)\tPrec@1 92.969 (93.703)\n",
            "Loss 0.2650 (0.1793)\tPrec@1 92.969 (93.636)\n",
            "Loss 0.2449 (0.1812)\tPrec@1 89.062 (93.538)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.680 \n",
            "EPOCH: 12\n",
            "TRAIN\n",
            "Loss 0.1049 (0.1049)\tPrec@1 98.438 (98.438)\n",
            "Loss 0.2300 (0.1722)\tPrec@1 93.750 (94.075)\n",
            "Loss 0.1401 (0.1726)\tPrec@1 96.875 (94.018)\n",
            "Loss 0.1394 (0.1706)\tPrec@1 95.312 (94.023)\n",
            "Loss 0.1677 (0.1755)\tPrec@1 94.531 (93.857)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.330 \n",
            "EPOCH: 13\n",
            "TRAIN\n",
            "Loss 0.1283 (0.1283)\tPrec@1 96.094 (96.094)\n",
            "Loss 0.1222 (0.1625)\tPrec@1 97.656 (94.400)\n",
            "Loss 0.1954 (0.1640)\tPrec@1 91.406 (94.201)\n",
            "Loss 0.2947 (0.1615)\tPrec@1 89.844 (94.256)\n",
            "Loss 0.1884 (0.1654)\tPrec@1 94.531 (94.196)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.550 \n",
            "EPOCH: 14\n",
            "TRAIN\n",
            "Loss 0.0978 (0.0978)\tPrec@1 98.438 (98.438)\n",
            "Loss 0.1708 (0.1579)\tPrec@1 96.094 (94.516)\n",
            "Loss 0.1019 (0.1573)\tPrec@1 96.875 (94.578)\n",
            "Loss 0.1149 (0.1608)\tPrec@1 95.312 (94.414)\n",
            "Loss 0.1891 (0.1609)\tPrec@1 92.969 (94.393)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.050 \n",
            "EPOCH: 15\n",
            "TRAIN\n",
            "Loss 0.1404 (0.1404)\tPrec@1 93.750 (93.750)\n",
            "Loss 0.0972 (0.1542)\tPrec@1 96.875 (94.640)\n",
            "Loss 0.1962 (0.1538)\tPrec@1 92.969 (94.671)\n",
            "Loss 0.1503 (0.1503)\tPrec@1 93.750 (94.832)\n",
            "Loss 0.1776 (0.1532)\tPrec@1 92.188 (94.668)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 91.130 \n",
            "EPOCH: 16\n",
            "TRAIN\n",
            "Loss 0.1847 (0.1847)\tPrec@1 95.312 (95.312)\n",
            "Loss 0.1879 (0.1334)\tPrec@1 91.406 (95.343)\n",
            "Loss 0.1149 (0.1373)\tPrec@1 97.656 (95.285)\n",
            "Loss 0.1528 (0.1409)\tPrec@1 94.531 (95.183)\n",
            "Loss 0.1656 (0.1421)\tPrec@1 92.969 (95.108)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.860 \n",
            "EPOCH: 17\n",
            "TRAIN\n",
            "Loss 0.1146 (0.1146)\tPrec@1 96.094 (96.094)\n",
            "Loss 0.0906 (0.1303)\tPrec@1 97.656 (95.699)\n",
            "Loss 0.2164 (0.1332)\tPrec@1 92.969 (95.390)\n",
            "Loss 0.2169 (0.1356)\tPrec@1 88.281 (95.268)\n",
            "Loss 0.1058 (0.1382)\tPrec@1 96.094 (95.176)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iwniQFSZwXu"
      },
      "source": [
        "We can see that by changing to a CNN for images we have gained a couple percent accuracy already. If you want to play around with this example you will be able to gain even more by modifying the network to include regularization methods such as dropout, augmenting or preprocessing your data, constructing larger and deeper models and finding better hyperparameters such as learning rates or mini-batch sizes.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDZzpmoGZwXv"
      },
      "source": [
        "### How well did the model do?\n",
        "In Machine Learning research it is crucial to compare and contrast a model to other researchers implementations. Many of the current Machine Learning datasets are posed as benchmarks where results are rigorously tracked in order to examine the efficiency and efficacy of a model or algorithm proposition.\n",
        "\n",
        "For the fashion MNIST dataset you can check how well both of your models (from scratch and in PyTorch) perform here:\n",
        "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#\n",
        "\n",
        "Do keep in mind that in order to analyze the usefulness of a method one should always compare and contrast on a variety of different datasets with varying task and complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVs6nQkR2sjs"
      },
      "source": [
        "---\n",
        "# Kuzushiji recognition - KMNIST: transferring what we have learned to a different task\n",
        "\n",
        "Here is where neural networks in combination with libraries such as PyTorch really begin to shine. Because the neural network approach is generic, our approach is transferable to a different classification task with minor code modifications. If the complexity of the target task is roughly the same, then we don't need to change the architecture and basically only need to exchange the dataloader.\n",
        "\n",
        "We will learn how to use PyTorch's in-built dataloaders for convenience and apply our learned knowledge to a second classification task: recognition of [classical Japanese handwritten Hiragana](https://github.com/rois-codh/kmnist).\n",
        "\n",
        "![Kuzushiji](https://raw.githubusercontent.com/rois-codh/kmnist/master/images/kmnist_examples.png)\n",
        "\n",
        "## KMNIST dataloader - PyTorch dataloaders and transformations for data augmentation\n",
        "Similar to above example where we wrap our custom dataset into PyTorch's data loaders, we will use PyTorch's in-built dataloaders. However, as the KMNIST dataset has already been added we can directly use convenience functions. We will also see how we can trivially implement data augmentation directly into our data loader.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVPFFpjQ4kBF"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "class KMNIST:\n",
        "    \"\"\"\n",
        "    KMNIST dataset featuring gray-scale 28x28 images of\n",
        "    Japanese Kuzushiji characters belonging to ten different classes.\n",
        "    Dataset implemented with torchvision.datasets.KMNIST.\n",
        "\n",
        "    Parameters:\n",
        "        args (dict): Dictionary of (command line) arguments.\n",
        "            Needs to contain batch_size (int) and workers(int).\n",
        "        is_gpu (bool): True if CUDA is enabled.\n",
        "            Sets value of pin_memory in DataLoader.\n",
        "\n",
        "    Attributes:\n",
        "        train_transforms (torchvision.transforms): Composition of transforms\n",
        "            including conversion to Tensor, repeating gray-scale image to\n",
        "            three channel for consistent use with different architectures\n",
        "            and normalization.\n",
        "        val_transforms (torchvision.transforms): Composition of transforms\n",
        "            including conversion to Tensor, repeating gray-scale image to\n",
        "            three channel for consistent use with different architectures\n",
        "            and normalization.\n",
        "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
        "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
        "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
        "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, is_gpu, batch_size=128, workers=4, patch_size=28):\n",
        "        self.num_classes = 10\n",
        "        self.patch_size = 28\n",
        "\n",
        "        self.train_transforms, self.val_transforms = self.__get_transforms()\n",
        "\n",
        "        self.trainset, self.valset = self.get_dataset()\n",
        "        self.train_loader, self.val_loader = self.get_dataset_loader(batch_size, workers, is_gpu)\n",
        "\n",
        "    def __get_transforms(self):\n",
        "        # We can define data transformations by composing a list of operations to execute\n",
        "        # this list of transformations can be given to the data loader and will be\n",
        "        # applied at every step of data loading. It is a really convenient way to\n",
        "        # implement random operations such as flips, translations, resizing etc.\n",
        "\n",
        "        # In below example we simply apply a resizing operation (to resize the\n",
        "        # images to whatever resolution is required for our architecture)\n",
        "        # and then convert the image to a tensor representation.\n",
        "        train_transforms = transforms.Compose([\n",
        "            transforms.Resize(size=(self.patch_size, self.patch_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        val_transforms = transforms.Compose([\n",
        "            transforms.Resize(size=(self.patch_size, self.patch_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        return train_transforms, val_transforms\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Uses torchvision.datasets.KMNIST to load dataset.\n",
        "        Downloads dataset if doesn't exist already.\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.TensorDataset: trainset, valset\n",
        "        \"\"\"\n",
        "        trainset = datasets.KMNIST('datasets/KMNIST/train/', train=True, transform=self.train_transforms,\n",
        "                                   target_transform=None, download=True)\n",
        "        valset = datasets.KMNIST('datasets/KMNIST/test/', train=False, transform=self.val_transforms,\n",
        "                                 target_transform=None, download=True)\n",
        "\n",
        "        return trainset, valset\n",
        "\n",
        "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
        "        \"\"\"\n",
        "        Defines the dataset loader for wrapped dataset\n",
        "\n",
        "        Parameters:\n",
        "            batch_size (int): Defines the batch size in data loader\n",
        "            workers (int): Number of parallel threads to be used by data loader\n",
        "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
        "\n",
        "        Returns:\n",
        "             torch.utils.data.DataLoader: train_loader, val_loader\n",
        "        \"\"\"\n",
        "\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            self.trainset,\n",
        "            batch_size=batch_size, shuffle=True,\n",
        "            num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            self.valset,\n",
        "            batch_size=batch_size, shuffle=False,\n",
        "            num_workers=workers, pin_memory=is_gpu)\n",
        "\n",
        "        return train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj2y_-H957kU"
      },
      "source": [
        "We have kept the global structure of the class the same. If we take a closer look we can however observe that both the `get_dataset` as well as the `get_dataset_loader` methods have essentially been replaced with single line calls to torchvision.\n",
        "\n",
        "We also no longer need to explicitly pre-load the entire dataset as before and convert it tensors. What we can do instead is we can define so called `transforms` that allows us to specify a sequence of operations that are executed on every loaded batch. This way we can trivially implement deterministic transforms such as conversion of images to tensors or stochastic data augmentation (think of random flips or translations to virtually augment the amount of different samples seen during training).\n",
        "\n",
        "This data loader will now only load a single mini-batch at a time and save us a lot of memory, which is essential if our dataset is too large to be loaded directly. For efficiency PyTorch has implemented this data loading in a multi-threaded version.\n",
        "\n",
        "## Train KMNIST\n",
        "We can directly use this now to train our models for recognition of ancient Japanese hiragana."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGQv3uhC5zz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7a20ca-11e0-46eb-d2c7-44a227ea16f0"
      },
      "source": [
        "# dataset\n",
        "dataset = KMNIST(is_gpu)\n",
        "\n",
        "# new model instance\n",
        "model = CNN(num_classes).to(device)\n",
        "\n",
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# optimize\n",
        "total_epochs = 20\n",
        "for epoch in range(total_epochs):\n",
        "    print(\"EPOCH:\", epoch + 1)\n",
        "    print(\"TRAIN\")\n",
        "    train(dataset.train_loader, model, criterion, optimizer, device)\n",
        "    print(\"VALIDATION\")\n",
        "    validate(dataset.val_loader, model, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to datasets/KMNIST/train/KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 18165135/18165135 [00:11<00:00, 1516358.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/train/KMNIST/raw/train-images-idx3-ubyte.gz to datasets/KMNIST/train/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to datasets/KMNIST/train/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 29497/29497 [00:00<00:00, 337138.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/train/KMNIST/raw/train-labels-idx1-ubyte.gz to datasets/KMNIST/train/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to datasets/KMNIST/train/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3041136/3041136 [00:01<00:00, 1784110.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/train/KMNIST/raw/t10k-images-idx3-ubyte.gz to datasets/KMNIST/train/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to datasets/KMNIST/train/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5120/5120 [00:00<00:00, 21389279.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/train/KMNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/KMNIST/train/KMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-images-idx3-ubyte.gz to datasets/KMNIST/test/KMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 18165135/18165135 [00:09<00:00, 1994012.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/test/KMNIST/raw/train-images-idx3-ubyte.gz to datasets/KMNIST/test/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/train-labels-idx1-ubyte.gz to datasets/KMNIST/test/KMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 29497/29497 [00:00<00:00, 339367.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/test/KMNIST/raw/train-labels-idx1-ubyte.gz to datasets/KMNIST/test/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-images-idx3-ubyte.gz to datasets/KMNIST/test/KMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 3041136/3041136 [00:01<00:00, 1698262.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/test/KMNIST/raw/t10k-images-idx3-ubyte.gz to datasets/KMNIST/test/KMNIST/raw\n",
            "\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://codh.rois.ac.jp/kmnist/dataset/kmnist/t10k-labels-idx1-ubyte.gz to datasets/KMNIST/test/KMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 5120/5120 [00:00<00:00, 6640332.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting datasets/KMNIST/test/KMNIST/raw/t10k-labels-idx1-ubyte.gz to datasets/KMNIST/test/KMNIST/raw\n",
            "\n",
            "EPOCH: 1\n",
            "TRAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 2.3236 (2.3236)\tPrec@1 5.469 (5.469)\n",
            "Loss 0.6099 (1.2302)\tPrec@1 82.031 (60.605)\n",
            "Loss 0.4561 (0.8607)\tPrec@1 84.375 (73.053)\n",
            "Loss 0.1688 (0.6926)\tPrec@1 96.094 (78.496)\n",
            "Loss 0.2193 (0.5848)\tPrec@1 92.969 (81.936)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 83.630 \n",
            "EPOCH: 2\n",
            "TRAIN\n",
            "Loss 0.1456 (0.1456)\tPrec@1 97.656 (97.656)\n",
            "Loss 0.3104 (0.2010)\tPrec@1 92.969 (94.152)\n",
            "Loss 0.0978 (0.1919)\tPrec@1 97.656 (94.275)\n",
            "Loss 0.1861 (0.1838)\tPrec@1 93.750 (94.539)\n",
            "Loss 0.1388 (0.1776)\tPrec@1 96.094 (94.740)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 88.260 \n",
            "EPOCH: 3\n",
            "TRAIN\n",
            "Loss 0.1255 (0.1255)\tPrec@1 96.094 (96.094)\n",
            "Loss 0.0972 (0.1318)\tPrec@1 96.875 (96.094)\n",
            "Loss 0.0792 (0.1260)\tPrec@1 98.438 (96.366)\n",
            "Loss 0.0927 (0.1236)\tPrec@1 97.656 (96.405)\n",
            "Loss 0.1053 (0.1224)\tPrec@1 96.875 (96.425)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.930 \n",
            "EPOCH: 4\n",
            "TRAIN\n",
            "Loss 0.1118 (0.1118)\tPrec@1 97.656 (97.656)\n",
            "Loss 0.1096 (0.0921)\tPrec@1 96.094 (97.285)\n",
            "Loss 0.0532 (0.0976)\tPrec@1 98.438 (97.085)\n",
            "Loss 0.0701 (0.0991)\tPrec@1 96.875 (96.989)\n",
            "Loss 0.0726 (0.0978)\tPrec@1 97.656 (97.058)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 90.890 \n",
            "EPOCH: 5\n",
            "TRAIN\n",
            "Loss 0.1462 (0.1462)\tPrec@1 95.312 (95.312)\n",
            "Loss 0.1891 (0.0883)\tPrec@1 96.875 (97.401)\n",
            "Loss 0.0690 (0.0861)\tPrec@1 99.219 (97.563)\n",
            "Loss 0.0352 (0.0840)\tPrec@1 99.219 (97.571)\n",
            "Loss 0.0660 (0.0842)\tPrec@1 97.656 (97.555)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 92.480 \n",
            "EPOCH: 6\n",
            "TRAIN\n",
            "Loss 0.0638 (0.0638)\tPrec@1 97.656 (97.656)\n",
            "Loss 0.0377 (0.0666)\tPrec@1 97.656 (98.089)\n",
            "Loss 0.0929 (0.0681)\tPrec@1 96.094 (97.979)\n",
            "Loss 0.0453 (0.0687)\tPrec@1 99.219 (97.978)\n",
            "Loss 0.0672 (0.0704)\tPrec@1 99.219 (97.919)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 92.150 \n",
            "EPOCH: 7\n",
            "TRAIN\n",
            "Loss 0.1076 (0.1076)\tPrec@1 96.875 (96.875)\n",
            "Loss 0.0520 (0.0572)\tPrec@1 97.656 (98.252)\n",
            "Loss 0.0324 (0.0612)\tPrec@1 100.000 (98.115)\n",
            "Loss 0.0452 (0.0610)\tPrec@1 99.219 (98.168)\n",
            "Loss 0.1453 (0.0599)\tPrec@1 98.438 (98.229)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 92.440 \n",
            "EPOCH: 8\n",
            "TRAIN\n",
            "Loss 0.0177 (0.0177)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.1274 (0.0470)\tPrec@1 96.094 (98.693)\n",
            "Loss 0.0803 (0.0485)\tPrec@1 98.438 (98.640)\n",
            "Loss 0.0508 (0.0510)\tPrec@1 97.656 (98.536)\n",
            "Loss 0.0212 (0.0535)\tPrec@1 100.000 (98.500)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.020 \n",
            "EPOCH: 9\n",
            "TRAIN\n",
            "Loss 0.0300 (0.0300)\tPrec@1 99.219 (99.219)\n",
            "Loss 0.0427 (0.0453)\tPrec@1 99.219 (98.731)\n",
            "Loss 0.0650 (0.0484)\tPrec@1 99.219 (98.640)\n",
            "Loss 0.0567 (0.0499)\tPrec@1 98.438 (98.591)\n",
            "Loss 0.0399 (0.0480)\tPrec@1 99.219 (98.644)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.240 \n",
            "EPOCH: 10\n",
            "TRAIN\n",
            "Loss 0.0236 (0.0236)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0270 (0.0443)\tPrec@1 99.219 (98.817)\n",
            "Loss 0.0290 (0.0416)\tPrec@1 98.438 (98.877)\n",
            "Loss 0.0724 (0.0426)\tPrec@1 98.438 (98.850)\n",
            "Loss 0.0334 (0.0432)\tPrec@1 98.438 (98.815)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.500 \n",
            "EPOCH: 11\n",
            "TRAIN\n",
            "Loss 0.0277 (0.0277)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0505 (0.0338)\tPrec@1 99.219 (99.134)\n",
            "Loss 0.0237 (0.0351)\tPrec@1 99.219 (99.052)\n",
            "Loss 0.1125 (0.0374)\tPrec@1 97.656 (99.006)\n",
            "Loss 0.0298 (0.0376)\tPrec@1 100.000 (98.991)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 92.980 \n",
            "EPOCH: 12\n",
            "TRAIN\n",
            "Loss 0.0203 (0.0203)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0458 (0.0378)\tPrec@1 97.656 (99.002)\n",
            "Loss 0.0328 (0.0367)\tPrec@1 99.219 (99.017)\n",
            "Loss 0.0384 (0.0353)\tPrec@1 99.219 (99.027)\n",
            "Loss 0.0557 (0.0358)\tPrec@1 97.656 (99.038)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.530 \n",
            "EPOCH: 13\n",
            "TRAIN\n",
            "Loss 0.0323 (0.0323)\tPrec@1 99.219 (99.219)\n",
            "Loss 0.0389 (0.0297)\tPrec@1 98.438 (99.281)\n",
            "Loss 0.0167 (0.0315)\tPrec@1 100.000 (99.160)\n",
            "Loss 0.0427 (0.0314)\tPrec@1 98.438 (99.162)\n",
            "Loss 0.0345 (0.0316)\tPrec@1 99.219 (99.137)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.870 \n",
            "EPOCH: 14\n",
            "TRAIN\n",
            "Loss 0.0231 (0.0231)\tPrec@1 99.219 (99.219)\n",
            "Loss 0.0424 (0.0257)\tPrec@1 99.219 (99.412)\n",
            "Loss 0.0762 (0.0259)\tPrec@1 97.656 (99.366)\n",
            "Loss 0.0707 (0.0282)\tPrec@1 98.438 (99.297)\n",
            "Loss 0.0394 (0.0290)\tPrec@1 99.219 (99.254)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.530 \n",
            "EPOCH: 15\n",
            "TRAIN\n",
            "Loss 0.0295 (0.0295)\tPrec@1 98.438 (98.438)\n",
            "Loss 0.0204 (0.0253)\tPrec@1 99.219 (99.404)\n",
            "Loss 0.0455 (0.0264)\tPrec@1 99.219 (99.355)\n",
            "Loss 0.0080 (0.0267)\tPrec@1 100.000 (99.333)\n",
            "Loss 0.0350 (0.0263)\tPrec@1 98.438 (99.345)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 94.070 \n",
            "EPOCH: 16\n",
            "TRAIN\n",
            "Loss 0.0090 (0.0090)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0067 (0.0218)\tPrec@1 100.000 (99.513)\n",
            "Loss 0.0175 (0.0233)\tPrec@1 100.000 (99.413)\n",
            "Loss 0.0226 (0.0237)\tPrec@1 99.219 (99.393)\n",
            "Loss 0.0379 (0.0243)\tPrec@1 99.219 (99.379)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.650 \n",
            "EPOCH: 17\n",
            "TRAIN\n",
            "Loss 0.0178 (0.0178)\tPrec@1 99.219 (99.219)\n",
            "Loss 0.0418 (0.0183)\tPrec@1 98.438 (99.590)\n",
            "Loss 0.0284 (0.0203)\tPrec@1 99.219 (99.534)\n",
            "Loss 0.0157 (0.0213)\tPrec@1 100.000 (99.507)\n",
            "Loss 0.0345 (0.0220)\tPrec@1 99.219 (99.484)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.910 \n",
            "EPOCH: 18\n",
            "TRAIN\n",
            "Loss 0.0079 (0.0079)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0144 (0.0175)\tPrec@1 100.000 (99.698)\n",
            "Loss 0.0055 (0.0199)\tPrec@1 100.000 (99.615)\n",
            "Loss 0.0093 (0.0207)\tPrec@1 100.000 (99.590)\n",
            "Loss 0.0196 (0.0213)\tPrec@1 100.000 (99.556)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.760 \n",
            "EPOCH: 19\n",
            "TRAIN\n",
            "Loss 0.0149 (0.0149)\tPrec@1 99.219 (99.219)\n",
            "Loss 0.0356 (0.0166)\tPrec@1 97.656 (99.698)\n",
            "Loss 0.0126 (0.0161)\tPrec@1 100.000 (99.716)\n",
            "Loss 0.0119 (0.0169)\tPrec@1 100.000 (99.702)\n",
            "Loss 0.0100 (0.0184)\tPrec@1 100.000 (99.618)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.740 \n",
            "EPOCH: 20\n",
            "TRAIN\n",
            "Loss 0.0240 (0.0240)\tPrec@1 100.000 (100.000)\n",
            "Loss 0.0267 (0.0168)\tPrec@1 99.219 (99.722)\n",
            "Loss 0.0317 (0.0162)\tPrec@1 98.438 (99.708)\n",
            "Loss 0.0112 (0.0175)\tPrec@1 100.000 (99.655)\n",
            "Loss 0.0161 (0.0189)\tPrec@1 100.000 (99.591)\n",
            "VALIDATION\n",
            " * Validation accuracy: Prec@1 93.930 \n"
          ]
        }
      ]
    }
  ]
}
